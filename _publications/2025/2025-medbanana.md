---
title:          "Med-Banana-50K: A Large-Scale Cross-Modality Dataset for Medical Image Editing"
title_zh:       "Med-Banana-50K: 大规模跨模态医学图像编辑数据集"
date:           2025-11-01 00:01:00 +0800
selected:       true
pub:            "arXiv preprint"
pub_zh:         "arXiv 预印本"
pub_date:       "2025"
abstract: >-
  Recent advances in multimodal large language models have enabled remarkable medical image editing capabilities. However, the research community's progress remains constrained by the absence of large-scale, high-quality, and openly accessible datasets built specifically for medical image editing with strict anatomical and clinical constraints. We introduce Med-Banana-50K, a comprehensive 50K-image dataset for instruction-based medical image editing spanning three modalities (chest X-ray, brain MRI, fundus photography) and 23 disease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image to generate bidirectional edits (lesion addition and removal) from real medical images. What distinguishes Med-Banana-50K from general-domain editing datasets is our systematic approach to medical quality control: we employ LLM-as-Judge with a medically grounded rubric and history-aware iterative refinement up to five rounds.
abstract_zh: >-
  近年来，多模态大语言模型的进步使医学图像编辑能力显著提升。然而，研究界的进展仍受限于缺乏专门为医学图像编辑构建的大规模、高质量且开放可访问的数据集，这些数据集需满足严格的解剖和临床约束。我们推出了 Med-Banana-50K，一个包含 50K 张图像的综合数据集，用于基于指令的医学图像编辑，涵盖三种模态（胸部 X 光、脑部 MRI、眼底照相）和 23 种疾病类型。我们的数据集通过利用 Gemini-2.5-Flash-Image 从真实医学图像生成双向编辑（病变添加和移除）来构建。Med-Banana-50K 的独特之处在于我们系统化的医学质量控制方法：采用基于医学评分准则的 LLM-as-Judge 和历史感知迭代优化（最多五轮）。
cover:          /assets/images/covers/medbanana_cover.jpg
authors:
  - Zhihui Chen
  - et al.
links:
  Paper: https://arxiv.org/submit/6942444/view
  Code: https://github.com/richardChenzhihui/med-banana-50k/
  Project Page: /showcase.html#projects
links_zh:
  论文: https://arxiv.org/submit/6942444/view
  代码: https://github.com/richardChenzhihui/med-banana-50k/
  项目页面: /showcase.html#projects
---

<!-- Note: arXiv link needs to be updated to the official publication link once available -->

